{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Build a Search Agent\n",
    "\n",
    "In this lab, we'll use the Azure AI Agent Service to create an agent that is able to retrieve information from documents stored in Azure AI Search, a vector database. This pattern is known as retrieval augmented generation or RAG. The documents that we'll be searching are finance data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Create the Azure AI Search Index\n",
    "\n",
    "We'll start the lab by creating an Azure AI Search index, which will contain vectorized representations of our finance documents. The steps shown below to create the AI Search index are from the official Microsoft [documentation](https://learn.microsoft.com/en-us/azure/search/search-get-started-portal-import-vectors?tabs=sample-data-storage%2Cmodel-aoai%2Cconnect-data-storage)\n",
    "\n",
    "#### Step 1\n",
    "To begin, download the [documents](https://github.com/jakeatmsft/fsi-azure-ai-agents-lab/tree/main/labs/data) that we'll store in the Azure AI Search index.\n",
    "\n",
    "#### Step 2\n",
    "Next, we'll upload the data documents to Azure Blob Storage, which Azure AI search connects to.\n",
    "1. Navigate to your Storage Account (the one that was created automaticall during your AI Foundry project setup). \n",
    "2. Expand \"Data Storage\" in the side menu and click on \"Containers\". \n",
    "3. Create a new container named \"financedata\"\n",
    "4. Click into the new container and upload the documents you downloaded.\n",
    "5. If you are not able to access blob storage to upload documents the enable public access with your ip to upload documents\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3\n",
    "We need an embedding model in order to convert our documents into vectors that will be stored in Azure AI Search...luckily, we've already deployed a  `text-embedding-3-large` model in Lab 1!\n",
    "\n",
    "#### Step 4\n",
    "Now we're ready to vectorize our documents.\n",
    "\n",
    "1. Go to your AOAI Service where your  `text-embedding-3-large` model is deployed and grant AI search Cognitive Services OpenAI User role.\n",
    "1. DO not use AI foundry project as it only support cohere and facbook embeddings models.\n",
    "1. On the **Overview page**, select **Import and vectorize data**.\n",
    "1. On **Set up your data connection**, select **Azure Blob Storage**.\n",
    "1. Specify your subscription, storage account, and the container that contains your finance data. \n",
    "1. Make sure **Authenticate using managed idenity is checked** and the **Managed identity type** is set to **System-assigned**.\n",
    "\n",
    "5. On the **Vectorize your text** page, select **Azure OpenAI** for **Kind**, select your subscription, and select the name of your Azure OpenAI Service.\n",
    "6. For the **Model deployment** select `text-embedding-3-large`. \n",
    "7. The **Authentication type** should be set to **System-assigned identity**. \n",
    "8. Select the box next to the acknowledgement.\n",
    "\n",
    "    \n",
    "9. You can hit **Next** for the next two pages. When you get to the **Review and create** page, set **Objects name prefix** to **financedata-index**. \n",
    "10. Click **Create**. This will start the document indexing process which will vectorize your documents and create an index.\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import AzureAISearchTool\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationAgent:\n",
    "    \"\"\"\n",
    "    A class to represent the Validation Agent.\n",
    "    \"\"\"\n",
    "    @kernel_function(description='An agent that runs validation checks to ensure that the generated analyst report meets required standards.')\n",
    "    def validate_report(self, report: str) -> str:\n",
    "        \"\"\"\n",
    "        Validates the generated analyst report.\n",
    "        Requirement: The report must include a detailed risk assessment.\n",
    "        \n",
    "        Parameters:\n",
    "        report (str): The analyst report produced by the AnalystReportAgent.\n",
    "        \n",
    "        Returns:\n",
    "        last_msg (json): The final message containing the validation result.\n",
    "        \"\"\"\n",
    "        print(\"Calling ValidationAgent...\")\n",
    "        \n",
    "        project_client = AIProjectClient.from_connection_string(\n",
    "            credential=DefaultAzureCredential(),\n",
    "            conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"],\n",
    "        )\n",
    "        \n",
    "        validation_agent = project_client.agents.create_agent(\n",
    "            model=\"gpt-4o\",\n",
    "            name=\"validation-agent\",\n",
    "            instructions=\"You are an expert agent that validates analyst reports. Return 'Pass' if the report includes a detailed risk assessment and meets all reporting standards, otherwise return 'Fail'. You must only return 'Pass' or 'Fail'.\",\n",
    "        )\n",
    "        \n",
    "        thread = project_client.agents.create_thread()\n",
    "        \n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=f\"Validate that the generated analyst report includes a detailed risk assessment. Here is the report: {report}\",\n",
    "        )\n",
    "        \n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=validation_agent.id)\n",
    "        \n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "        \n",
    "        project_client.agents.delete_agent(validation_agent.id)\n",
    "        \n",
    "        messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "        last_msg = messages.get_last_text_message_by_role(\"assistant\")\n",
    "        \n",
    "        print(\"ValidationAgent completed successfully.\")\n",
    "        return last_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Create the Search Agent\n",
    "\n",
    "Now that we've vectorized our documents and created an index, we can create a Search Agent that will retreive information about our data from the index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import AzureAISearchTool\n",
    "\n",
    "load_dotenv() # Loads the environment variables and credentials we need to setup the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Connect to your Azure AI Foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to our Azure AI Foundry project, which will allow us to use the deployed gpt-4o model\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Connect to your Azure AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection ID: /subscriptions/ff38e682-afe0-4ec2-8e18-d14de894978f/resourceGroups/agents-lab-rg/providers/Microsoft.MachineLearningServices/workspaces/agent-lab-project/connections/AzureAISearch\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the connections in your project and get the connection ID of the Aazure AI Search connection.\n",
    "conn_list = project_client.connections.list()\n",
    "conn_id = \"\"\n",
    "for conn in conn_list:\n",
    "    if conn.connection_type == \"CognitiveSearch\":\n",
    "        conn_id = conn.id\n",
    "        print(f\"Connection ID: {conn.id}\")\n",
    "\n",
    "# Connect to your Azure AI Search index\n",
    "ai_search = AzureAISearchTool(index_connection_id=conn_id, index_name=\"financedata-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Define the search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    name=\"search-agent\",\n",
    "    instructions=\"You are a helpful agent that is an expert at searching documents.\",\n",
    "    tools=ai_search.definitions,\n",
    "    tool_resources=ai_search.resources,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Chat with the search agent\n",
    "\n",
    "Let's test our search agent by asking it to give us information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: The Velo product appears to be associated with varying sectors, including government, midmarket, enterprise, and channel partnerships. It includes different pricing tiers depending on the market segment. For instance:\n",
      "\n",
      "- Manufacturing prices typically start at $120 and sale prices range from $7 to $350, depending on discounts or market focus【3:0†source】【3:1†source】【3:2†source】.\n",
      "- Sales data show its broad adoption across countries like Germany, Mexico, Canada, and France【3:3†source】【3:0†source】.\n",
      "\n",
      "If further specific details or context are needed, feel free to let me know! \n",
      "\n",
      "{'type': 'text', 'text': {'value': 'The Velo product appears to be associated with varying sectors, including government, midmarket, enterprise, and channel partnerships. It includes different pricing tiers depending on the market segment. For instance:\\n\\n- Manufacturing prices typically start at $120 and sale prices range from $7 to $350, depending on discounts or market focus【3:0†source】【3:1†source】【3:2†source】.\\n- Sales data show its broad adoption across countries like Germany, Mexico, Canada, and France【3:3†source】【3:0†source】.\\n\\nIf further specific details or context are needed, feel free to let me know!', 'annotations': [{'type': 'url_citation', 'text': '【3:0†source】', 'start_index': 343, 'end_index': 355, 'url_citation': {'url': 'financial_sample.xlsx', 'title': 'financial_sample.xlsx'}}, {'type': 'url_citation', 'text': '【3:1†source】', 'start_index': 355, 'end_index': 367, 'url_citation': {'url': 'financial_sample.xlsx', 'title': 'financial_sample.xlsx'}}, {'type': 'url_citation', 'text': '【3:2†source】', 'start_index': 367, 'end_index': 379, 'url_citation': {'url': 'financial_sample.xlsx', 'title': 'financial_sample.xlsx'}}, {'type': 'url_citation', 'text': '【3:3†source】', 'start_index': 475, 'end_index': 487, 'url_citation': {'url': 'financial_sample.xlsx', 'title': 'financial_sample.xlsx'}}, {'type': 'url_citation', 'text': '【3:0†source】', 'start_index': 487, 'end_index': 499, 'url_citation': {'url': 'financial_sample.xlsx', 'title': 'financial_sample.xlsx'}}]}}\n"
     ]
    }
   ],
   "source": [
    "# The name of the product we want to search for\n",
    "product_name = 'Velo'\n",
    "\n",
    "\n",
    "# Create a thread which is a conversation session between an agent and a user. \n",
    "thread = project_client.agents.create_thread()\n",
    "\n",
    "# Create a message in the thread with the user asking for information about a specific product\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=f\"Tell me about the {product_name} product.\", # The user's message\n",
    ")\n",
    "# Run the agent to process tne message in the thread\n",
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=search_agent.id)\n",
    "\n",
    "# Check if the run was successful\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Delete the agent when it's done running\n",
    "#project_client.agents.delete_agent(search_agent.id)\n",
    "\n",
    "# Fetch all the messages from the thread\n",
    "messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "\n",
    "# Get the last message, which is the agent's resposne to the user's question\n",
    "last_msg = messages.get_last_text_message_by_role(\"assistant\")\n",
    "\n",
    "# Display the agent's response\n",
    "print('Agent:', last_msg.text.value, '\\n')\n",
    "print(last_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
